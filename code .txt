import streamlit as st
import subprocess
import pandas as pd
import os
import re
import glob
import pickle
import torch
import tempfile
import gdown
from transformers import AutoTokenizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# ==============================
# Konfigurasi Halaman
# ==============================
st.set_page_config(page_title="Twitter Sentiment App", page_icon="üìä", layout="wide")
st.title("üìä Twitter Sentiment Analysis")

# ==============================
# Session State
# ==============================
if "selected_keyword" not in st.session_state:
    st.session_state.selected_keyword = ""
if "filename" not in st.session_state:
    st.session_state.filename = None
if "switch_to_tab" not in st.session_state:
    st.session_state.switch_to_tab = None

# ==============================
# Custom CSS 
# ==============================
st.markdown("""
<style>
.stApp {
    background: linear-gradient(135deg, #d4fc79, #96e6a1);
    font-family: 'Segoe UI', sans-serif;
    color: #222;
}
h1 { text-align: center; color: #1b4332; font-weight: 700; }
h2 { color: #2d6a4f; }
div[data-testid="stSidebar"] {
    background: linear-gradient(180deg, #ffffff, #f9f9f9);
    padding: 20px; border-radius: 16px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.05);
}
button[kind="primary"] {
    background: linear-gradient(90deg, #43cea2, #185a9d) !important;
    border-radius: 12px !important;
    color: white !important; font-weight: bold !important; border: none !important;
}
button[kind="secondary"] {
    border-radius: 8px !important;
    background-color: #2d6a4f !important;
    color: white !important;
    font-weight: 600 !important; font-size: 14px !important;
    transition: all 0.2s ease;
}
button[kind="secondary"]:hover { background-color: #40916c !important; transform: scale(1.02); }

/* ====== Spacing tombol keyword 5px hanya di dalam container beranda ====== */
.beranda-keyword-row [data-testid="column"] { padding-left: 0 !important; padding-right: 5px !important; }
.beranda-keyword-row [data-testid="column"]:last-child { padding-right: 0 !important; }

[data-testid="stToolbar"] {display: none;}
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
header {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# ==============================
# JavaScript untuk auto-switch tab
# ==============================
js_code = """
<script>
function switchTab(tabIndex){
    const tabs = window.parent.document.querySelectorAll('button[data-baseweb="tab"]');
    if (tabs && tabs.length > tabIndex){
        tabs[tabIndex].click();
    }
}
</script>
"""
st.markdown(js_code, unsafe_allow_html=True)

# ==============================
# Tabs 
# ==============================
tabs = st.tabs(["üè† Beranda", "üì• Ambil Data", "üìä Analisis & Visualisasi"])

# Jika ada flag pindah tab dari event sebelumnya, jalankan JS
if st.session_state.switch_to_tab is not None:
    st.markdown(f"<script>switchTab({int(st.session_state.switch_to_tab)});</script>", unsafe_allow_html=True)
    st.session_state.switch_to_tab = None  # reset

# ==============================
# Tools & Stopwords
# ==============================
factory = StemmerFactory()
stemmer = factory.create_stemmer()
stop_words = set([
    'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk', 'dengan',
    'pada', 'juga', 'karena', 'ada', 'tidak', 'sudah', 'saja', 'lebih',
    'akan', 'bagi', 'para', 'sebagai', 'oleh', 'tentang', 'maka', 'atau',
    'jadi', 'namun'
])

def clean_text(text):
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"http\S+|www\S+|pic.twitter\S+", "", text)
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"\d+", "", text)
    return text.lower().strip()

def remove_stopwords(text):
    return " ".join([word for word in text.split() if word not in stop_words])

def tokenize_text(text):
    return text.split()

allowed_keywords = [
    "kebijakan makanan siang gratis",
    "makan siang bergizi di sekolah",
    "program makanan bergizi"
]

def filter_by_keywords(text):
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in allowed_keywords)

# ==============================
# Load Model 
# ==============================
@st.cache_resource
def load_local_model():
    try:
        file_id = "105CSGfijs7MTSg41o6fNBAlLpRri3j6E"
        drive_url = f"https://drive.google.com/uc?id={file_id}"
        temp_path = os.path.join(tempfile.gettempdir(), "model_temp.pkl")

        if not os.path.exists(temp_path):
            with st.spinner("üì• Mengunduh model dari Google Drive..."):
                gdown.download(drive_url, temp_path, quiet=False)

        with open(temp_path, "rb") as f:
            model = pickle.load(f)

        if not hasattr(model.config, "output_attentions"):
            model.config.output_attentions = False
        if not hasattr(model.config, "output_hidden_states"):
            model.config.output_hidden_states = False

        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        return model, tokenizer
    except Exception as e:
        st.error(f"‚ùå Gagal memuat model lokal: {e}")
        return None, None

local_model, local_tokenizer = load_local_model()

def is_berita_comment(text):
    if re.search(r"http\S+|www\S+|pic.twitter\S+", text):
        return True
    if "#" in text:
        return True
    return False

def predict_sentiment_local(texts):
    results = []
    if local_model and local_tokenizer:
        for txt in texts:
            if is_berita_comment(txt):
                results.append("Netral")
                continue
            inputs = local_tokenizer(txt, return_tensors="pt", padding=True, truncation=True)
            with torch.no_grad():
                outputs = local_model(**inputs)
                predictions = torch.argmax(outputs.logits, dim=1)
                label_map = {0: "Negatif", 1: "Positif", 2: "Netral"}
                results.append(label_map[predictions.item()])
    else:
        results = ["Netral"] * len(texts)
    return results

# ==============================
# Tab 1: Beranda
# ==============================
with tabs[0]:
    st.markdown("""
    <div style="padding:20px; background:white; border-radius:12px; box-shadow:0 4px 10px rgba(0,0,0,0.1);">
        <h2>Selamat Datang di Aplikasi Twitter Sentiment Analysis üìä</h2>
        <p>Aplikasi ini dirancang untuk mengumpulkan data dari Twitter berdasarkan kata kunci tertentu, 
        kemudian melakukan <b>analisis sentimen</b> terhadap tweet tersebut. Hasil analisis akan divisualisasikan
        dalam bentuk <b>diagram pie</b> dan <b>WordCloud</b>.</p>
        <h3>üîç Fitur Utama:</h3>
        <ul>
            <li>Ambil data tweet berdasarkan kata kunci.</li>
            <li>Preprocessing teks.</li>
            <li>Analisis sentimen menggunakan model berbasis AI.</li>
            <li>Visualisasi sentimen dan WordCloud.</li>
            <li>Download hasil analisis dalam format CSV.</li>
        </ul>
        <h3>üìò Panduan Penggunaan:</h3>
        <ol>
            <li>Pilih salah satu kata kunci di bawah.</li>
            <li>Aplikasi otomatis pindah ke tab <b>Ambil Data</b> dan siap mengunduh tweet.</li>
        </ol>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("<h3 style='margin-top:20px;'>üéØ Pilih Kata Kunci Cepat:</h3>", unsafe_allow_html=True)

    # Jika keyword sudah dipilih, tampilkan notifikasi + tombol X
    if "selected_keyword" in st.session_state and st.session_state.selected_keyword:
        st.markdown(f"""
        <div style="padding:10px; background:#e8f4ff; border-radius:8px; margin-bottom:10px; display:flex; justify-content:space-between; align-items:center; border:1px solid #b3d8ff;">
            <span style="font-size:16px; color:#333;">‚úÖ Kata kunci terpilih: <b>{st.session_state.selected_keyword}</b></span>
        </div>
        """, unsafe_allow_html=True)

        # Tombol X untuk hapus
        if st.button("‚ùå Ganti Kata Kunci"):
            st.session_state.selected_keyword = None
            st.session_state.switch_to_tab = 0
            st.rerun()

    # Tampilkan pilihan keyword jika belum dipilih
    if not st.session_state.get("selected_keyword"):
        st.markdown('<div class="beranda-keyword-row" style="display:flex; gap:10px;">', unsafe_allow_html=True)
        cols = st.columns(len(allowed_keywords))
        for idx, kw in enumerate(allowed_keywords):
            with cols[idx]:
                if st.button(kw, key=f"kw_{idx}", use_container_width=True):
                    st.session_state.selected_keyword = kw
                    st.session_state.switch_to_tab = 1
                    st.rerun()
        st.markdown('</div>', unsafe_allow_html=True)

# ==============================
# Tab 2: Ambil Data
# ==============================
with tabs[1]:
    st.subheader("üì• Ambil Data dari Twitter")

    # Tampilkan kata kunci yang dipilih (tanpa input manual)
    if st.session_state.selected_keyword:
        st.markdown(f"**Kata Kunci Terpilih:** `{st.session_state.selected_keyword}`")
    else:
        st.warning("‚ö†Ô∏è Belum ada kata kunci yang dipilih. Silakan kembali ke tab Beranda.")

    limit = st.number_input("üìå Limit jumlah tweet", min_value=10, max_value=1000, value=100, step=10)

    output_folder = "tweets-data"
    os.makedirs(output_folder, exist_ok=True)
    twitter_auth_token = "7c61b36f86f134a53c742564e0f592960648b33c"

    if st.button("üì• Ambil Data dari Twitter"):
        if not st.session_state.selected_keyword:
            st.warning("‚ö†Ô∏è Pilih kata kunci terlebih dahulu di tab Beranda.")
        else:
            command = f"npx tweet-harvest -o {output_folder} -s \"{st.session_state.selected_keyword}\" --tab LATEST -l {limit} --token {twitter_auth_token}"
            try:
                with st.spinner("üì• Mengambil data dari Twitter..."):
                    result = subprocess.run(command, shell=True, text=True, capture_output=True)
                if result.returncode == 0:
                    csv_files = glob.glob(os.path.join(output_folder, "*.csv"))
                    if csv_files:
                        st.session_state.filename = max(csv_files, key=os.path.getctime)
                        st.success("‚úÖ Data berhasil diambil.")
                    else:
                        st.error("‚ùå CSV tidak ditemukan setelah proses pengambilan.")
                else:
                    st.error(f"‚ùå Error saat mengambil data:\n{result.stderr}")
            except Exception as e:
                st.error(f"‚ùå Terjadi error:\n{str(e)}")

# ==============================
# Tab 3: Analisis & Visualisasi
# ==============================
with tabs[2]:
    data_folder = "tweets-data"
    
    # Ambil file terbaru jika belum ada di session
    if not st.session_state.get("filename"):
        csv_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith(".csv")]
        if csv_files:
            latest_file = max(csv_files, key=os.path.getctime)
            st.session_state.filename = latest_file

    # Proses analisis
    if st.session_state.filename and os.path.exists(st.session_state.filename):
        df = pd.read_csv(st.session_state.filename)
        if df.empty:
            st.warning("‚ö†Ô∏è Tidak ditemukan tweet yang sesuai kata/frasa kunci.")
        else:
            st.subheader("üìã Data Mentah")
            st.dataframe(df, use_container_width=True, height=500)

            # Cari kolom teks paling relevan
            text_columns = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 10]
            if text_columns:
                selected_col = "full_text" if "full_text" in text_columns else text_columns[0]

                if st.button("üìä Analisis Data"):
                    with st.spinner("‚è≥ Sedang memproses..."):
                        # Preprocessing
                        df["clean_text"]   = df[selected_col].apply(clean_text)
                        df["no_stopwords"] = df["clean_text"].apply(remove_stopwords)
                        df["tokenized"]    = df["no_stopwords"].apply(tokenize_text)
                        df["stemmed"]      = df["tokenized"].apply(lambda tokens: " ".join([stemmer.stem(word) for word in tokens]))

                        # Prediksi sentimen
                        df["sentiment"] = predict_sentiment_local(df[selected_col].tolist())

                        # Tabel hasil
                        st.subheader("üìÑ Hasil Preprocessing & Sentimen")
                        st.dataframe(df[[selected_col, "clean_text", "no_stopwords", "tokenized", "stemmed", "sentiment"]],
                                     use_container_width=True)

                        # Pie chart sentimen
                        st.subheader("üìà Visualisasi Sentimen")
                        sentiment_counts = df["sentiment"].value_counts().reset_index()
                        sentiment_counts.columns = ["sentiment", "count"]

                        fig = px.pie(sentiment_counts, values="count", names="sentiment",
                                     title="Distribusi Sentimen", hole=0.3)
                        fig.update_layout(template="plotly_dark",
                                          paper_bgcolor="rgba(0,0,0,0)",
                                          plot_bgcolor="rgba(0,0,0,0)",
                                          font=dict(size=18))
                        st.plotly_chart(fig, use_container_width=True)

                        # WordCloud per sentimen
                        st.subheader("‚òÅÔ∏è WordCloud per Sentimen")
                        for label in df["sentiment"].unique():
                            label_text = " ".join(df[df["sentiment"] == label]["stemmed"])
                            if label_text.strip():
                                wordcloud = WordCloud(width=800, height=400, background_color="white",
                                                      max_words=100, colormap="Greens").generate(label_text)
                                fig_wc, ax = plt.subplots(figsize=(10, 5))
                                ax.imshow(wordcloud, interpolation="bilinear")
                                ax.axis("off")
                                ax.set_title(f"WordCloud - Sentimen: {label}",
                                             fontsize=16, fontweight="bold", pad=20)
                                st.pyplot(fig_wc)

                        # Download CSV
                        st.download_button("‚¨áÔ∏è Download Hasil Sentimen",
                                           df.to_csv(index=False),
                                           "hasil_sentimen.csv",
                                           "text/csv")
            else:
                st.warning("‚ö†Ô∏è Tidak ditemukan kolom teks yang valid.")
    else:
        st.info("‚ÑπÔ∏è Belum ada data yang tersedia di folder 'tweets-data'.")
